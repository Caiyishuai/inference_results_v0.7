diff --git a/ilit/adaptor/mxnet.py b/ilit/adaptor/mxnet.py
index 2ca779a..944de6b 100644
--- a/ilit/adaptor/mxnet.py
+++ b/ilit/adaptor/mxnet.py
@@ -76,11 +76,6 @@ class MxNetAdaptor(Adaptor):
             (dict): quantized model
         """
         assert q_func is None, "quantization aware training mode is not support on mxnet"
-        self.cfg = tune_cfg
-        self._cfg_to_qconfig(tune_cfg)
-        self.__config_dict['calib_data'] = dataloader
-        self.th_dict = None
-        qconfig = self.__config_dict
 
         # get symbol from FP32 model
         if isinstance(model, mx.gluon.HybridBlock):
@@ -88,12 +83,18 @@ class MxNetAdaptor(Adaptor):
             sym, arg_params, aux_params, calib_data = \
                 self._get_gluon_symbol(model, dataloader=dataloader)
             data_names = [pair[0] for pair in calib_data.provide_data]
+            self.__config_dict['calib_data'] = calib_data
         elif isinstance(model[0], mx.symbol.Symbol):
             sym, arg_params, aux_params = model
+            self.__config_dict['calib_data'] = dataloader
         else:
             raise ValueError(
                 'Need a symbol model or HybridBlock model, while received %s' % str(
                     type(model)))
+
+        self._cfg_to_qconfig(tune_cfg)
+        self.th_dict = None
+        qconfig = self.__config_dict
         sym = self._get_backedn_graph(sym, qconfig['ctx'])
 
         # 1. quantize_symbol
@@ -213,7 +214,7 @@ class MxNetAdaptor(Adaptor):
             output = output[0].asnumpy()
             if postprocess is not None:
                 output = postprocess(output)
-            label = batch.label[0].asnumpy()
+            label = (batch.label[0] + 1).asnumpy()
             if metric is not None:
                 metric.update(output, label)
             batch_num += dataIter.batch_size
@@ -575,17 +576,16 @@ class MxNetAdaptor(Adaptor):
         # for not tunable config
         quantized_dtype = 'auto'
         quantize_mode = 'smart'
-        quantize_granularity = 'channel-wise'
+        quantize_granularity = 'tensor-wise'
         logger = None
         ctx = mx.cpu()
-        calib_data = None
-        batch_size = 64
+        batch_size = self.__config_dict['calib_data'].batch_size
         iteration = 2
         if 'calib_iteration' in list(tune_cfg.keys()):
             iteration = tune_cfg['calib_iteration']
         num_calib_examples = batch_size * iteration
 
-        self.__config_dict = {
+        self.__config_dict.update({
             "excluded_sym_names": excluded_sym_names,
             "excluded_op_names": excluded_op_names,
             "LayerOutputCollector": LayerOutputCollector,
@@ -594,13 +594,12 @@ class MxNetAdaptor(Adaptor):
             "quantize_granularity": quantize_granularity,
             "logger": logger,
             "ctx": ctx,
-            "calib_data": calib_data,
             "num_calib_examples": num_calib_examples,
             "iteration": iteration,
             "exclude_layers_match": [],
             "calib_kl_layers": calib_kl_layers,
             "calib_minmax_layers": calib_minmax_layers,
-        }
+        })
 
     def _get_gluon_symbol(self, network, dataloader):
         """Convert symbol model and DataIter from gluon model HybridBlock/Dataloader.
@@ -801,6 +800,11 @@ class MxNetAdaptor(Adaptor):
             if logger:
                 logger.info('Collected layer output KL values from FP32 model')
 
+        for data_name in data_names:
+            assert data_name in layer_tensor.keys(), "input name: %s not in inspect tensors!" % data_name
+            self.__config_dict["calib_minmax_layers"].append(data_name)
+
+        calib_data.reset()
         if len(self.__config_dict["calib_minmax_layers"]) != 0:
             th_dict_minmax, num_examples = mx.contrib.quantization._collect_layer_output_min_max(
                 mod, calib_data, quantized_dtype, include_layer=self.__config_dict[
diff --git a/ilit/metric/metric.py b/ilit/metric/metric.py
index 03204dd..ae7ca64 100644
--- a/ilit/metric/metric.py
+++ b/ilit/metric/metric.py
@@ -237,7 +237,7 @@ class TopK(Metric):
     def update(self, preds, labels, sample_weight=None):
 
         preds = preds.argsort()[..., -self.k:]
-        preds = np.squeeze(preds)
+        preds = preds.reshape(labels.shape)
         if self.k == 1:
             correct = accuracy_score(preds, labels, normalize=False)
             self.num_correct += correct
